{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaolivei/Perceptron_porta_logica_OR/blob/main/Aula_8_Percetron_Porta_Logica_OR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Perceptron e a porta lógica OR</h1>\n",
        "\n"
      ],
      "metadata": {
        "id": "AcOdSoosgPuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* O Perceptron é um tipo de rede neural artificial inventada em 1957 por Frank Rosenblatt. Essa rede é capaz de realizar classificações binárias e pode ser usado para modelar funções lógicas simples, como a função OR.\n",
        "* O Perceptron de uma camada consiste basicamente em um neurônio com vários sinais de entrada e um sinal de saída. O treinamento de um Perceptron envolve ajustar os pesos das conexões de entrada para que o neurônio produza a saída correta para as entradas fornecidas.\n",
        "* Abaixo temos um exemplo simples de como implementar e treinar um Perceptron de uma camada para modelar a função lógica OR usando Python. A função OR é verdadeira (1) se qualquer uma de suas entradas é verdadeira (1), e falsa (0) caso contrário."
      ],
      "metadata": {
        "id": "jj83bry0ghvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Importação da Biblioteca Numpy</h2>"
      ],
      "metadata": {
        "id": "odwQQ-SlgmBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IdnvURBNkkV"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Criação da Classe Perceptron</h2>"
      ],
      "metadata": {
        "id": "rccPrZojhFd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* O código abaixe define a classe Perceptron com métodos para ajustar o modelo aos dados (fit) e fazer previsões (predict). O treinamento ajusta os pesos e o bias com base nas entradas e saídas esperadas fornecidas, utilizando uma taxa de aprendizado para determinar a magnitude das atualizações de peso em cada época. Após o treinamento, o modelo pode prever a saída para qualquer entrada da função lógica OR.\n",
        "* Uma época é uma passagem completa pelo conjunto de treinamento, onde o algoritmo tenta ajustar os pesos e o bias com base em todas as amostras fornecidas. Durante cada época, o modelo itera sobre todo o conjunto de dados de treinamento, fazendo ajustes nos pesos e no bias para cada amostra, na tentativa de minimizar o erro entre as previsões e as saídas reais."
      ],
      "metadata": {
        "id": "yAldYPtchLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicialização: Os pesos e o bias são inicializados, geralmente com valores pequenos aleatórios ou zeros.\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_epochs=10):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Inicializa os pesos e o bias com zeros\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Treinamento\n",
        "        for iteration in range(self.n_epochs):\n",
        "            print(f\"\\nIteração {iteration + 1}/{self.n_epochs}\")\n",
        "            print(\"-------------------------\")\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Calcula a saída do perceptron\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_function(linear_output)\n",
        "\n",
        "                # Mostra o estado antes da atualização\n",
        "                print(f\"Entrada: {x_i}, Saída esperada: {y[idx]}, Saída prevista: {y_predicted}\")\n",
        "                print(f\"Pesos antes da atualização: {self.weights}, Bias antes: {self.bias}\")\n",
        "\n",
        "                # Atualiza os pesos e o bias\n",
        "                update = self.learning_rate * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "                # Mostra o estado após a atualização\n",
        "                print(f\"Pesos após a atualização: {self.weights}, Bias após: {self.bias}\\n\")\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        # Função de ativação: 1 se x > 0, senão 0\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_function(linear_output)\n",
        "        return y_predicted"
      ],
      "metadata": {
        "id": "noLY9JfISdxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Conjunto de dados para o treinamento</h2>"
      ],
      "metadata": {
        "id": "bquIBDznhvUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de entrada e saídas esperadas para a porta OR\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) #valores de entrada da porta lógica OR\n",
        "y = np.array([0, 1, 1, 1]) #resultado da aplicação da lógica da porta OR\n",
        "print(\"Valores de entrada da porta lógica OR:\\n\", X)\n",
        "print(\"Resultado da porta lógica OR:\\n\", y)"
      ],
      "metadata": {
        "id": "BxguA92NSgM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578734db-2b5d-4250-8527-7f4cdb83f04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores de entrada da porta lógica OR:\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Resultado da porta lógica OR:\n",
            " [0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Treinamento do Perceptron com o Conjunto de dados</h2>"
      ],
      "metadata": {
        "id": "TeiXbvQeiI2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No exemplo abaixo, foi configurada uma taxa de aprendizado de 0.1 e n_epochs de 10, o que significa que o algoritmo passou 10 vezes por todo o conjunto de treinamento, ajustando os parâmetros (pesos e bias) a cada passagem para aprender a função lógica OR. A escolha do número de épocas depende de vários fatores, incluindo a complexidade do problema, o tamanho do conjunto de dados e a velocidade de convergência do modelo. Um número muito baixo de épocas pode resultar em um modelo subajustado, enquanto um número excessivamente alto pode levar a um desperdício de recursos computacionais ou até mesmo a um ajuste excessivo, onde o modelo aprende padrões específicos do conjunto de treinamento que não se generalizam bem para novos dados."
      ],
      "metadata": {
        "id": "epyPTJ_twnHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o perceptron\n",
        "perceptron = Perceptron(learning_rate=0.1, n_epochs=2)\n",
        "perceptron.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKFVQQ3XSsxb",
        "outputId": "8503ea35-cec6-4906-922d-03e3f2329431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteração 1/2\n",
            "-------------------------\n",
            "Entrada: [0 0], Saída esperada: 0, Saída prevista: 0\n",
            "Pesos antes da atualização: [0. 0.], Bias antes: 0\n",
            "Pesos após a atualização: [0. 0.], Bias após: 0.0\n",
            "\n",
            "Entrada: [0 1], Saída esperada: 1, Saída prevista: 0\n",
            "Pesos antes da atualização: [0. 0.], Bias antes: 0.0\n",
            "Pesos após a atualização: [0.  0.1], Bias após: 0.1\n",
            "\n",
            "Entrada: [1 0], Saída esperada: 1, Saída prevista: 1\n",
            "Pesos antes da atualização: [0.  0.1], Bias antes: 0.1\n",
            "Pesos após a atualização: [0.  0.1], Bias após: 0.1\n",
            "\n",
            "Entrada: [1 1], Saída esperada: 1, Saída prevista: 1\n",
            "Pesos antes da atualização: [0.  0.1], Bias antes: 0.1\n",
            "Pesos após a atualização: [0.  0.1], Bias após: 0.1\n",
            "\n",
            "\n",
            "Iteração 2/2\n",
            "-------------------------\n",
            "Entrada: [0 0], Saída esperada: 0, Saída prevista: 1\n",
            "Pesos antes da atualização: [0.  0.1], Bias antes: 0.1\n",
            "Pesos após a atualização: [0.  0.1], Bias após: 0.0\n",
            "\n",
            "Entrada: [0 1], Saída esperada: 1, Saída prevista: 1\n",
            "Pesos antes da atualização: [0.  0.1], Bias antes: 0.0\n",
            "Pesos após a atualização: [0.  0.1], Bias após: 0.0\n",
            "\n",
            "Entrada: [1 0], Saída esperada: 1, Saída prevista: 0\n",
            "Pesos antes da atualização: [0.  0.1], Bias antes: 0.0\n",
            "Pesos após a atualização: [0.1 0.1], Bias após: 0.1\n",
            "\n",
            "Entrada: [1 1], Saída esperada: 1, Saída prevista: 1\n",
            "Pesos antes da atualização: [0.1 0.1], Bias antes: 0.1\n",
            "Pesos após a atualização: [0.1 0.1], Bias após: 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Testando o modelo gerado pelo Perceptron</h12>"
      ],
      "metadata": {
        "id": "Q0ux9-JXiap-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa o perceptron treinado\n",
        "print(\"\\nPrevisões finais do Perceptron para as entradas da porta OR:\")\n",
        "for input_data, target in zip(X, y):\n",
        "    prediction = perceptron.predict(input_data.reshape(1, -1))\n",
        "    print(f\"Entrada: {input_data}, Saída esperada: {target}, Saída prevista: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACAZP9qJSy8B",
        "outputId": "747f4030-bc06-4afd-b66f-f09160e595f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Previsões finais do Perceptron para as entradas da porta OR:\n",
            "Entrada: [0 0], Saída esperada: 0, Saída prevista: [1]\n",
            "Entrada: [0 1], Saída esperada: 1, Saída prevista: [1]\n",
            "Entrada: [1 0], Saída esperada: 1, Saída prevista: [1]\n",
            "Entrada: [1 1], Saída esperada: 1, Saída prevista: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Visualização dos pesos e bias</h2>"
      ],
      "metadata": {
        "id": "4z6008Uwi4az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testa o perceptron treinado e imprime os pesos após o treinamento\n",
        "print(\"\\nPesos após o treinamento:\", perceptron.weights)\n",
        "print(\"Bias após o treinamento:\", perceptron.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx2bxuctS3hZ",
        "outputId": "3c5a7d93-8d1a-4012-80b0-890441932595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos após o treinamento: [0.1 0.1]\n",
            "Bias após o treinamento: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Exercício</h1>"
      ],
      "metadata": {
        "id": "JGgI5ssrxI81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Análise o código que fez o treinamento do modelo para aprender a porta lógica OR e avalie se o número de épocas = 10 foi necessário ou se o modelo conseguiria aprender com um número de épocas menor. Em caso positivo qual a quantidade de épocas necessárias para o modelo aprender sem desperdiçar recursos computacionais?"
      ],
      "metadata": {
        "id": "9D6aQ5WQxJtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clique duas vezes __aqui__ para obter a resposta.\n",
        "<!--\n",
        "Na terceira época, percebe-se que os pesos e o bias estão estabilizados ([0.1, 0.1] para os pesos e 0.0 para o bias), indicando que o modelo não estava mais fazendo ajustes. Isso sugere que o Perceptron já havia aprendido a representar a função lógica OR corretamente com esses parâmetros na época 3.\n",
        "\n",
        "Portanto, em termos de aprendizado da função lógica OR, três épocas teriam sido suficientes para o Perceptron ajustar seus parâmetros adequadamente para essa tarefa específica. Este é um exemplo de como a complexidade do problema e a capacidade do modelo influenciam o número de épocas necessário para o aprendizado. Para problemas simples como a representação de funções lógicas com um Perceptron de uma camada, poucas épocas são geralmente suficientes. Porém, para problemas mais complexos ou modelos mais sofisticados, podem ser necessárias mais épocas para alcançar uma aprendizagem adequada.\n",
        "-->"
      ],
      "metadata": {
        "id": "GCb5T7_xx9M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Agora altere esse código para fazer o Perceptron aprender a porta lógica AND considerando o número de épocas igual a 10."
      ],
      "metadata": {
        "id": "-dCbVp6pyMt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clique duas vezes __aqui__ para obter a resposta..\n",
        "\n",
        "<!--\n",
        "#Você precisa alterar a trecho do \"Conjunto de dados para o treinamento\" e #na saída Y colocar os valores lógicos da porta AND\n",
        "y = np.array([0, 0, 0, 1]) ##resultado da aplicação da lógica da porta AND\n",
        "#teste com o número de epocas = 10\n",
        "-->"
      ],
      "metadata": {
        "id": "PjjhkeP5yrLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Análise o código que fez o treinamento do modelo para aprender a porta lógica AND e avalie se o número de épocas = 10 foi necessário ou se o modelo conseguiria aprender com um número de épocas menor. Em caso positivo qual a quantidade de épocas necessárias para o modelo aprender sem desperdiçar recursos computacionais?"
      ],
      "metadata": {
        "id": "N88EVhWyI5na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clique duas vezes __aqui__ para obter a resposta.\n",
        "<!--\n",
        "Na quinta época, percebe-se que os pesos e o bias estão estabilizados ([0.2 0.1] para os pesos e -0.2 para o bias), indicando que o modelo não estava mais fazendo ajustes. Isso sugere que o Perceptron já havia aprendido a representar a função lógica AND corretamente com esses parâmetros na época 5.\n",
        "\n",
        "Portanto, em termos de aprendizado da função lógica AND, cinco épocas teriam sido suficientes para o Perceptron ajustar seus parâmetros adequadamente para essa tarefa específica. Este é um exemplo de como a complexidade do problema e a capacidade do modelo influenciam o número de épocas necessário para o aprendizado. Para problemas simples como a representação de funções lógicas com um Perceptron de uma camada, poucas épocas são geralmente suficientes. Porém, para problemas mais complexos ou modelos mais sofisticados, podem ser necessárias mais épocas para alcançar uma aprendizagem adequada.\n",
        "-->"
      ],
      "metadata": {
        "id": "2-1buVUYKgIp"
      }
    }
  ]
}